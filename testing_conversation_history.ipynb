{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26656a89-aa9a-4cb0-ba75-a8407c487cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from config import settings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2136e4-f72b-4ba4-8f45-7d83d8b3cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEO4J_URI = settings.NEO4J_URI\n",
    "NEO4J_USERNAME = settings.NEO4J_USERNAME\n",
    "NEO4J_PASSWORD = settings.NEO4J_PASSWORD\n",
    "OPENAI_API_KEY = settings.OPENAI_API_KEY\n",
    "OPENAI_ENDPOINT = settings.OPENAI_ENDPOINT or \"https://api.openai.com/v1\"\n",
    "TMDB_API_KEY = settings.TMDB_API_KEY\n",
    "OMDB_API = settings.OMDB_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76c0395-7e3c-42b3-9edd-1da76ff04251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NEO4j is a graph database management system designed to store, manage, and query highly connected data. It is based on the concept of a graph, which consists of nodes (representing entities) and relationships (representing connections between entities). These nodes and relationships can have properties associated with them, making it a flexible and powerful tool for modeling complex and interconnected data. It uses the Cypher query language, which allows for intuitive and efficient querying of data in the graph. NEO4j is used in various industries, including finance, healthcare, and social media, for tasks such as fraud detection, recommendation engines, and network analysis. \n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "response= llm.invoke(\"What is NEO4j\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e041cd35-09bc-4c16-8de5-db99ccd1929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, mate, that's a right juicy Adam and Eve you got there. Perfect for a nice pie or crumble, innit?\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate(template=\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\", input_variables={\"fruit\"})\n",
    "\n",
    "response = llm.invoke(template.format(fruit=\"apple\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46529917-cb55-4887-89a9-f379c6a233b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring the LLM\n",
    "llm = OpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature = 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1179d7-ff93-40f8-aac7-dc1c48f06ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, mate, that's a right juicy Adam and Eve you got there. Perfect for a nice pie or crumble, innit?\n"
     ]
    }
   ],
   "source": [
    "## chaining\n",
    "\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "llm_chain = template | llm # this is called chaining\n",
    "\n",
    "response = llm_chain.invoke({\"fruit\":\"apple\"})\n",
    "\n",
    "#another way of chaining\n",
    "#response = llm.invoke(template.format(fruit=\"apple\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "227c8a8b-37b2-4371-9729-21ba863a8aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"type\": \"Apples and pears, mate. They're the bee's knees of fruit. Crunchy and juicy, they are!\"}\n"
     ]
    }
   ],
   "source": [
    "# adding output parser\n",
    "\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Output JSON as {{\"description\": \"your response here\"}}\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "llm_chain = template | llm | StrOutputParser()\n",
    "response = llm_chain.invoke({\"fruit\":\"apple\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b2277-e4ff-4b77-815f-b799e333741a",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef249e87-6e51-4e37-b708-9538a065cea7",
   "metadata": {},
   "source": [
    "### Create a Chat Model\n",
    "\n",
    "A language model predicts the next word in a sequence of words. Chat models are designed to have conversations - they accept a list of messages and return a conversational response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "677b08e1-187b-4f90-a0e1-468c97d9b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the weather is totally gnarly today! The sun is shining, the waves are pumping, and the vibes are just totally epic. It's going to be a sick day out on the water, for sure! Are you hitting the waves today too, bro?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage \n",
    "chat_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY) #see the difference in the object function\n",
    "\n",
    "instructions = SystemMessage(content=\"\"\"\n",
    "You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\"\"\")\n",
    "\n",
    "question = HumanMessage(content=\"What is the weather like?\")\n",
    "response = chat_llm.invoke([\n",
    "    instructions,\n",
    "    question\n",
    "])\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae91b4-6d60-4c65-b0c7-5429ae028fbc",
   "metadata": {},
   "source": [
    "### Wrapping Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0be590c-f7d6-45f9-b507-d01425999d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the weather is epic! Clear skies, offshore winds, and the waves are firing! It's gonna be a sick sesh out there today! Stoked to hit the waves, brah!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "response = chat_chain.invoke({\"question\": \"What is the weather like?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7b809-081d-47f1-ac05-23c22e3e9caa",
   "metadata": {},
   "source": [
    "### Giving context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9003b536-4b91-4803-ae82-0efcf4d26b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, Watergate Bay is firing with 3ft waves right now, but the wind is a bit of a bummer with onshore winds.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        ( \"system\", \"{context}\" ),\n",
    "        ( \"human\", \"{question}\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc124d-94c2-41a2-89ce-d10e132369f8",
   "metadata": {},
   "source": [
    "### Giving a Chat Model Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b66e773-eff4-47a6-9739-999a459ffeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  how is the wether in Ireland\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the surf at Fistral is firing! We've got some sick 6ft waves and offshore winds, it's gonna be epic out there! If you're looking for some mellow vibes, maybe head to Bells - it's flat and calm over there. Or if you're up for a challenge, Watergate Bay is pumping with 3ft waves and onshore winds. Choose your beach wisely, bro!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return memory\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "#giving history\n",
    "\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Bells\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "while (question := input(\"> \")) != \"exit\":\n",
    "\n",
    "    response = chat_with_message_history.invoke(\n",
    "        {\n",
    "            \"context\": current_weather,\n",
    "            \"question\": question,\n",
    "            \n",
    "        }, \n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"none\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf10540c-117b-4ade-bfbe-ab8196993740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 2f7cd6a0-0808-4994-81d3-27c020adb17c\n"
     ]
    }
   ],
   "source": [
    "## session id \n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979898e-2840-4f49-980d-db25c2a70c49",
   "metadata": {},
   "source": [
    "### Connecting to the NEO4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f76562cf-27c9-426f-b62d-fedfc6621a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'m.tagline': \"Twelve outrageous guests. Four scandalous requests. And one lone bellhop, in his first day on the job, who's in for the wildest New year's Eve of his life.\"}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (m:Movie {title: 'Four Rooms'})\n",
    "RETURN m.tagline\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e95c91ed-e7be-46bb-bc17-6066c00ebf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Movie {title: STRING, tagline: STRING, taglineEmbedding: LIST, tmdb_id: INTEGER, overview: STRING, release_date: DATE, poster_path: STRING, poster_url: STRING, trailer_url: STRING, popularity: FLOAT, vote_average: FLOAT, vote_count: INTEGER, last_updated: DATE_TIME, poster_embedding: LIST}\n",
      "Person {name: STRING, tmdb_id: INTEGER, gender: INTEGER, last_updated: DATE_TIME}\n",
      "Relationship properties:\n",
      "ACTED_IN {roles: LIST}\n",
      "The relationships:\n",
      "(:Person)-[:ACTED_IN]->(:Movie)\n",
      "(:Person)-[:DIRECTED]->(:Movie)\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafb693-cfe0-498b-98a3-a062240598c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32d94337",
   "metadata": {},
   "source": [
    "## Adding chat History using session_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 7c8d7dd9-a49d-42b1-8ccb-277e8778c1bf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_neo4j import Neo4jChatMessageHistory\n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "chat_with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Bells\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "while (question := input(\"> \")) != \"exit\":\n",
    "    \n",
    "    response = chat_with_message_history.invoke(\n",
    "        {\n",
    "            \"context\": current_weather,\n",
    "            \"question\": question,\n",
    "            \n",
    "        }, \n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": SESSION_ID}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
